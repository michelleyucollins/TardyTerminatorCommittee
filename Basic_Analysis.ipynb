{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On this Jupyter Notebook \n",
    "Creation date: February 12, 2024\n",
    "\n",
    "For satisfactory Bus Delay Data store see \"./data/delays/bus-delay.csv\".\n",
    "\n",
    "This Jupyter Notebook is intended to perform basic data analysis on TTC delay data, with the goal of combining data from multiple excel spreadsheets and, within them, multiple sheets such that there is one usable CSV file containing a specific transportation system's delay data, ordered by their recorded Date and Time stamp. \n",
    "\n",
    "This Jupyter Notebook must be in the home directory to be run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bus_prefix = \"./data/delays/ttc-bus-delay-data-\"\n",
    "file_names = []\n",
    "for i in range(2016, 2025):\n",
    "    file_names.append(f\"{bus_prefix}{i}.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_all_sheets_to_csv(file_names, output_csv=\"combined.csv\"):\n",
    "    \"\"\"\n",
    "    Reads all sheets from every Excel file in file_names (without interpreting any row as header),\n",
    "    concatenates the data from all sheets vertically, and exports the combined data to a CSV file\n",
    "    without any column headers.\n",
    "\n",
    "    Parameters:\n",
    "        file_names (list of str): List of paths to Excel files.\n",
    "        output_csv (str): The path for the output CSV file.\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "\n",
    "    column_data = ['Report Date','Route','Time','Day','Location','Incident', 'Min Delay','Min Gap','Direction','Vehicle']\n",
    "\n",
    "    for file in file_names:\n",
    "        # Open the excel sheet\n",
    "        try:\n",
    "            excel_file = pd.ExcelFile(file)\n",
    "        except Exception as e:\n",
    "            print(f\"Error opening file '{file}': {e}\")\n",
    "            continue\n",
    "\n",
    "        # Read and append values for each sheet in the file\n",
    "        for sheet in excel_file.sheet_names:\n",
    "            try:\n",
    "                # Header is first row\n",
    "                dummy = pd.read_excel(excel_file, sheet_name=sheet, header=0)\n",
    "                dummy.columns = column_data\n",
    "                all_data.append(dummy)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading sheet '{sheet}' in file '{file}': {e}\")\n",
    "                continue\n",
    "\n",
    "    if not all_data:\n",
    "        print(\"There was no data in any file given or the passed file_name vector was empty\")\n",
    "        return\n",
    "\n",
    "    # Concat all the data vertically\n",
    "    combined_df = pd.concat(all_data, ignore_index=True)\n",
    "    combined_df.columns = column_data\n",
    "\n",
    "    try:\n",
    "        # Export to CSV without column headers and without the index.\n",
    "        combined_df.to_csv(output_csv, header=column_data, index=False)\n",
    "        print(f\"Combined data successfully exported to '{output_csv}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error exporting data to CSV: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined data successfully exported to './data/delays/bus-delay-data-2016-2024.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Combine all sheets to csv file and process the csv file\n",
    "combine_all_sheets_to_csv(file_names, output_csv=\"./data/delays/bus-delay-data-2016-2024.csv\")\n",
    "# !!!!!!!!!!!!!!!!!!DATE TIME PROCESSING MUST BE DONE LOCALLY IN EXCEL THIS NEARLY CRASHED MY COMPUTER!!!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please complete DateTime Processing MANUALLY in EXCEL before proceeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing_full_ttc_csv(file_name, dict_file_name = \"info.json\"): \n",
    "    '''\n",
    "    Function for processing TTC delay data stored in a CSV file. This function must be run prior to further analysis to allow for some\n",
    "    functions to run smoothly.\n",
    "    Manual grunt work may be needed for certain data cleaning, which is why print statements are added for some brief manual parsing\n",
    "\n",
    "    Input: file name of a TTC Delay CSV File; (optional) file name of a data_dict dump file, if empty it is defaulted to 'info.json'\n",
    "    Output: returns a dictionary of data for the specified file_name (unique categoricies for a feature, etc)\n",
    "\n",
    "    Note: the Data Dictionary is also stored as a JSON file for later use if needed\n",
    "    '''\n",
    "    df = pd.read_csv(file_name)\n",
    "    column_data = df.columns\n",
    "    data_dict = {}\n",
    "    for col in column_data:\n",
    "        df[col] = df[col].apply(lambda x: x.lower() if isinstance(x, str) else x)\n",
    "        if isinstance(df[col][1], str) and col != 'DateTime' and col != 'Report Date' and col != 'Time' and col != 'Day':\n",
    "            data_dict[col] = df[col].unique().tolist()\n",
    "    \n",
    "    df.to_csv(file_name, index='DateTime')\n",
    "    \n",
    "    with open(dict_file_name, \"w\") as outfile: \n",
    "        json.dump(data_dict, outfile)\n",
    "        \n",
    "    return data_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xt/5my4_t657l5dvcsk9ybkb_ww0000gn/T/ipykernel_54841/135831970.py:12: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_name)\n"
     ]
    }
   ],
   "source": [
    "data_dict = pre_processing_full_ttc_csv(\"./data/delays/bus-delay-final.csv\", dict_file_name= \"./data/delays/bus-delay-data.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On Bus delay data\n",
    "Observations of unique bus delay locations from preprocessed data (see \"./data/delays/bus-delay-data.json\") indicates an exorbant number of typos present, especially when compared to route station names in \"data/TTC Routes and Schedules Data/stops.txt\" (retrieved from https://open.toronto.ca/dataset/ttc-routes-and-schedules/). These files are pushed to github in a zip folder as they are very large. They are in the gitignore.\n",
    "\n",
    "Further processing (textual) is necessary for ease of analysis in future steps. As texts must be done in a 1-1 manner, a LLM processing method would be used, aligned with that as described in Lecture #10 with Gemini 2.0 Flash. \n",
    "\n",
    "Please see \"./preliminary/station_name_processing.ipynb\" for further details of this process.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "colab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
