{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On this Jupyter Notebook \n",
    "Creation date: February 12, 2024\n",
    "\n",
    "For satisfactory Bus Delay Data store see \"./data/delays/bus-delay-store.csv\".\n",
    "\n",
    "This Jupyter Notebook is intended to perform basic data analysis on TTC delay data, with the goal of combining data from multiple excel spreadsheets and, within them, multiple sheets such that there is one usable CSV file containing a specific transportation system's delay data, ordered by their recorded Date and Time stamp. \n",
    "\n",
    "This Jupyter Notebook must be in the home directory to be run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Update as of 2025-02-22 from Cindy: reorganized the direcotry and folders to distinguish bus and subway data. \n",
    "### Need to be careful with the file path.\n",
    "suwbay_prefix = \"./data/delays/subway/ttc-subway-delay-data-\"\n",
    "file_names = []\n",
    "for i in range(2016, 2025):\n",
    "    file_names.append(f\"{subway_prefix}{i}.xlsx\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Depending on which dataset is used, the function calls would be different. \n",
    "- The distinction is made because the column_data is different for bus and subway. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_all_sheets_to_csv(file_names, output_csv=\"combined.csv\"):\n",
    "    \"\"\"\n",
    "    Reads all sheets from every Excel file in file_names (without interpreting any row as header),\n",
    "    concatenates the data from all sheets vertically, and exports the combined data to a CSV file\n",
    "    without any column headers.\n",
    "\n",
    "    Parameters:\n",
    "        file_names (list of str): List of paths to Excel files.\n",
    "        output_csv (str): The path for the output CSV file.\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "\n",
    "    column_data = ['Report Date','Route','Time','Day','Location','Incident', 'Min Delay','Min Gap','Direction','Vehicle']\n",
    "\n",
    "    for file in file_names:\n",
    "        # Open the excel sheet\n",
    "        try:\n",
    "            excel_file = pd.ExcelFile(file)\n",
    "        except Exception as e:\n",
    "            print(f\"Error opening file '{file}': {e}\")\n",
    "            continue\n",
    "\n",
    "        # Read and append values for each sheet in the file\n",
    "        for sheet in excel_file.sheet_names:\n",
    "            try:\n",
    "                # Header is first row\n",
    "                dummy = pd.read_excel(excel_file, sheet_name=sheet, header=0)\n",
    "                dummy.columns = column_data\n",
    "                all_data.append(dummy)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading sheet '{sheet}' in file '{file}': {e}\")\n",
    "                continue\n",
    "\n",
    "    if not all_data:\n",
    "        print(\"There was no data in any file given or the passed file_name vector was empty\")\n",
    "        return\n",
    "\n",
    "    # Concat all the data vertically\n",
    "    combined_df = pd.concat(all_data, ignore_index=True)\n",
    "    combined_df.columns = column_data\n",
    "\n",
    "    try:\n",
    "        combined_df.to_csv(output_csv, header=column_data, index=False)\n",
    "        print(f\"Combined data successfully exported to '{output_csv}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error exporting data to CSV: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined data successfully exported to './data/delays/bus-delay-data-2016-2024.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Combine all sheets to csv file and process the csv file\n",
    "combine_all_sheets_to_csv(file_names, output_csv=\"./data/delays/bus-delay-data-2016-2024.csv\")\n",
    "# !!!!!!!!!!!!!!!!!!DATE TIME PROCESSING MUST BE DONE LOCALLY IN EXCEL THIS NEARLY CRASHED MY COMPUTER!!!!!!!!!!!!!!!!!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined data successfully exported to '/Users/jessica_1/Workspace/EngSci_Year3/ECE324/TardyTerminatorCommittee/data/delays/bus/subway/ttc-subway-delay-data-2014-2024.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def combine_subway_sheets_to_csv(file_names, output_csv=\"./data/delays/subway-delay-data-2016-2024.csv\"):\n",
    "    \"\"\"\n",
    "    Reads all sheets from every subway delay Excel file, concatenates them, and exports the combined data to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "        file_names (list of str): List of relative paths to Excel files.\n",
    "        output_csv (str): Relative path for the output CSV file.\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "\n",
    "    # Updated column names for the subway dataset\n",
    "    column_data = ['Date', 'Time', 'Day', 'Station', 'Code', 'Min Delay', 'Min Gap', 'Bound', 'Line', 'Vehicle']\n",
    "\n",
    "    for file in file_names:\n",
    "        try:\n",
    "            excel_file = pd.ExcelFile(file)\n",
    "        except Exception as e:\n",
    "            print(f\"Error opening file '{file}': {e}\")\n",
    "            continue\n",
    "\n",
    "        for sheet in excel_file.sheet_names:\n",
    "            try:\n",
    "                dummy = pd.read_excel(excel_file, sheet_name=sheet, header=0)\n",
    "                dummy.columns = column_data\n",
    "                all_data.append(dummy)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading sheet '{sheet}' in file '{file}': {e}\")\n",
    "                continue\n",
    "\n",
    "    if not all_data:\n",
    "        print(\"No data found in given files.\")\n",
    "        return\n",
    "\n",
    "    # Concatenate all sheets together\n",
    "    combined_df = pd.concat(all_data, ignore_index=True)\n",
    "    combined_df.columns = column_data\n",
    "\n",
    "    try:\n",
    "        combined_df.to_csv(output_csv, header=True, index=False)\n",
    "        print(f\"Combined data successfully exported to '{output_csv}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error exporting data to CSV: {e}\")\n",
    "\n",
    "subway_prefix = \"/Users/jessica_1/Workspace/EngSci_Year3/ECE324/TardyTerminatorCommittee/data/delays/bus/subway/ttc-subway-delay-data-\"\n",
    "file_names = [f\"{subway_prefix}{i}.xlsx\" for i in range(2016, 2025)]\n",
    "\n",
    "# Run the function \n",
    "combine_subway_sheets_to_csv(file_names, output_csv=\"/Users/jessica_1/Workspace/EngSci_Year3/ECE324/TardyTerminatorCommittee/data/delays/bus/subway/ttc-subway-delay-data-2014-2024.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please complete DateTime Processing MANUALLY in EXCEL before proceeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing_full_ttc_csv(file_name, dict_file_name = \"info.json\"): \n",
    "    '''\n",
    "    Function for processing TTC delay data stored in a CSV file. This function must be run prior to further analysis to allow for some\n",
    "    functions to run smoothly.\n",
    "    Manual grunt work may be needed for certain data cleaning, which is why print statements are added for some brief manual parsing\n",
    "\n",
    "    Input: file name of a TTC Delay CSV File; (optional) file name of a data_dict dump file, if empty it is defaulted to 'info.json'\n",
    "    Output: returns a dictionary of data for the specified file_name (unique categoricies for a feature, etc)\n",
    "\n",
    "    Note: the Data Dictionary is also stored as a JSON file for later use if needed\n",
    "    '''\n",
    "    df = pd.read_csv(\"/Users/jessica_1/Workspace/EngSci_Year3/ECE324/TardyTerminatorCommittee/data/delays/bus/subway/ttc-subway-delay-data-2014-2024 - ttc-subway-delay-data-2014-2024.csv\")\n",
    "    column_data = df.columns\n",
    "    data_dict = {}\n",
    "    for col in column_data:\n",
    "        df[col] = df[col].apply(lambda x: x.lower() if isinstance(x, str) else x)\n",
    "        if isinstance(df[col][1], str) and col != 'DateTime' and col != 'Report Date' and col != 'Time' and col != 'Day':\n",
    "            data_dict[col] = df[col].unique().tolist()\n",
    "    \n",
    "    # df.to_csv(file_name, index='DateTime')\n",
    "    \n",
    "    # with open(dict_file_name, \"w\") as outfile: \n",
    "    #     json.dump(data_dict, outfile)\n",
    "        \n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y2/255cmf8x457_1wfw5bhh8cwm0000gp/T/ipykernel_18494/135831970.py:12: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_name)\n"
     ]
    }
   ],
   "source": [
    "# for Bus Data\n",
    "data_dict = pre_processing_full_ttc_csv(\"./data/delays/bus-delay-final.csv\", dict_file_name= \"./data/delays/bus-delay-data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for subway data\n",
    "data_dict = pre_processing_full_ttc_csv(\"./data/delays/subway-delay-data-2016-2024.csv\", dict_file_name=\"./data/delays/subway-delay-data.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On Bus delay data\n",
    "Observations of unique bus delay locations from preprocessed data (see \"./data/delays/bus-delay-data.json\") indicates an exorbant number of typos present, especially when compared to route station names in \"data/TTC Routes and Schedules Data/stops.txt\" (retrieved from https://open.toronto.ca/dataset/ttc-routes-and-schedules/). These files are pushed to github in a zip folder as they are very large. They are in the gitignore.\n",
    "\n",
    "Further processing (textual) is necessary for ease of analysis in future steps. As texts must be done in a 1-1 manner, a LLM processing method would be used, aligned with that as described in Lecture #10 with Gemini 2.0 Flash. \n",
    "\n",
    "Please see \"./preliminary/stop_name_processing.py\" for further details of this process.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bus_station_names = pd.read_csv(\"./data/TTC Routes and Schedules Data/stops.txt\")\n",
    "bus_station_names = bus_station_names[\"stop_name\"]\n",
    "bus_station_names = pd.DataFrame(bus_station_names.unique())\n",
    "bus_station_names.to_csv(\"./data/TTC Routes and Schedules Data/stop_names_unique.txt\", index = False, header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bus_station_names = pd.read_csv(\"./data/TTC Routes and Schedules Data/stop_names_unique.txt\")\n",
    "bus_station_names = pd.DataFrame(bus_station_names[\"Name\"].unique())\n",
    "bus_station_names.to_csv(\"./data/TTC Routes and Schedules Data/stop_names_unique.txt\", index = False, header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./preliminary/results copy.txt\", 'r', encoding='utf-8') as f:\n",
    "    results = f.read()\n",
    "results = results.replace('\\n\\n','\\n')\n",
    "with open(\"./preliminary/results copy.txt\", 'w', encoding='utf-8') as f:\n",
    "    f.write(results)\n",
    "\n",
    "with open(\"./preliminary/results copy.txt\", \"r\", encoding=\"utf-8\") as original, \\\n",
    "    open(\"./preliminary/results bad.txt\", \"w\", encoding=\"utf-8\") as store, \\\n",
    "    open(\"./preliminary/results good.txt\", \"w\", encoding=\"utf-8\") as new:\n",
    "    for line in original:\n",
    "        if line.count(\",\") != 2:\n",
    "            store.write(line)\n",
    "        else:\n",
    "            new.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xt/5my4_t657l5dvcsk9ybkb_ww0000gn/T/ipykernel_42947/3305714624.py:2: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  bus_data = pd.read_csv(\"./data/delays/bus/bus-delay-feb28.csv\")\n"
     ]
    }
   ],
   "source": [
    "results = pd.read_csv(\"./preliminary/results good.txt\")\n",
    "bus_data = pd.read_csv(\"./data/delays/bus/bus-delay-feb28.csv\")\n",
    "bus_data[\"Location\"] = bus_data[\"Location\"].str.lower()\n",
    "results = results.drop_duplicates(subset=\"bad_name\", keep=\"first\")\n",
    "merged_df = bus_data.merge(results, left_on=\"Location\", right_on=\"bad_name\", how=\"left\")\n",
    "merged_df = merged_df.drop(columns=[\"bad_name\"])\n",
    "merged_df.to_csv(\"stops_corrected.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[\"good_name\"] = merged_df[\" good_name\"]\n",
    "merged_df = merged_df.drop(columns=[\" good_name\"])\n",
    "invalid_rows = merged_df[merged_df[\"good_name\"].isna() | merged_df[\"confidence\"].isna()]\n",
    "valid_rows = merged_df.dropna(subset=[\"good_name\", \"confidence\"])\n",
    "\n",
    "invalid_rows.to_csv(\"invalid_stops.csv\", index=False)\n",
    "valid_rows.to_csv(\"all_stops_corrected.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xt/5my4_t657l5dvcsk9ybkb_ww0000gn/T/ipykernel_42947/369572736.py:1: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  valid_bus_data = pd.read_csv(\"./data/bus_data_finish/all_stops_corrected.csv\")\n"
     ]
    }
   ],
   "source": [
    "valid_bus_data = pd.read_csv(\"./data/bus_data_finish/all_stops_corrected.csv\")\n",
    "stop_data = pd.read_csv(\"./data/TTC Routes and Schedules Data/stops.txt\")\n",
    "stop_data = stop_data[[\"stop_name\", \"stop_lat\", \"stop_lon\"]]\n",
    "stop_data = stop_data.drop_duplicates(subset=\"stop_name\", keep=\"first\")\n",
    "merged_df = valid_bus_data.merge(stop_data, left_on=\"good_name\", right_on=\"stop_name\", how=\"left\")\n",
    "merged_df = merged_df.drop(columns=[\"stop_name\"])\n",
    "merged_df.to_csv(\"./data/bus_data_finish/full_bus_data.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "colab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
